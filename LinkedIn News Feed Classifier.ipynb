{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b83a234",
   "metadata": {},
   "source": [
    "# LinkedIn news feed classifier\n",
    "\n",
    "### This Notebook takes your linkedin news feed and classifies it into 4 categories (Job lead,Event,Others and Ads)\n",
    "\n",
    "The notebook is divided into 3 parts:\n",
    "1. Import content\n",
    "2. Classify content\n",
    "3. Access the required category\n",
    "\n",
    "\n",
    "Importing of data is done using chromedriver. The latest version is available at https://sites.google.com/chromium.org/driver/\n",
    "\n",
    "## 1. Import content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575edc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports\n",
    "import time\n",
    "import pandas as pd\n",
    "import re as re\n",
    "import pickle\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from IPython.core.display import display, HTML \n",
    "from IPython.display import IFrame\n",
    "from sklearn.pipeline import Pipeline \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df55f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#See if existing user credential file exists or create one \n",
    "# Borrowed this cell from https://christophegaron.com/scraping-linkedin-posts-with-selenium-and-beautiful-soup/\n",
    "try:\n",
    "    f= open(\"linkedin_credentials.txt\",\"r\")\n",
    "    contents = f.read()\n",
    "    username = contents.replace(\"=\",\",\").split(\",\")[1]\n",
    "    password = contents.replace(\"=\",\",\").split(\",\")[3]\n",
    "except:\n",
    "    f= open(\"linkedin_credentials.txt\",\"w+\")\n",
    "    username = input('Enter your linkedin username: ')\n",
    "    password = input('Enter your linkedin password: ')\n",
    "    f.write(\"username={}, password={}\".format(username,password))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d40604",
   "metadata": {},
   "outputs": [],
   "source": [
    "#accessing Chromedriver\n",
    "browser = webdriver.Chrome('chromedriver')\n",
    "\n",
    "#Open login page\n",
    "browser.get('https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')\n",
    "\n",
    "#Enter login info:\n",
    "elementID = browser.find_element_by_id('username')\n",
    "elementID.send_keys(username)\n",
    "\n",
    "elementID = browser.find_element_by_id('password')\n",
    "elementID.send_keys(password)\n",
    "elementID.submit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd9eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simulate scrolling to capture all posts\n",
    "SCROLL_PAUSE_TIME = 1.5\n",
    "#Set the pages number counter\n",
    "PAGES=0\n",
    "#Set maximum number of pages to extract\n",
    "MAX_PAGES=30\n",
    "# Get scroll height\n",
    "last_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while PAGES<MAX_PAGES:\n",
    "    # Scroll down to bottom\n",
    "    browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "    PAGES+=1\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d423cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check out page source code\n",
    "company_page = browser.page_source  \n",
    "\n",
    "\n",
    "#Use Beautiful Soup to get access tags\n",
    "linkedin_soup = bs(company_page.encode(\"utf-8\"), \"html\")\n",
    "linkedin_soup.prettify()\n",
    "\n",
    "#Find the post blocks\n",
    "containers = linkedin_soup.findAll(attrs={\"data-urn\": True}) #gets content\n",
    "print(len(containers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928bc08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removes other types of contents to get required html content\n",
    "updated_containers=[x for x in containers if '<div class=\"feed-shared-text relative feed-shared-update-v2__commentary\" dir=\"ltr\">' in str(x)] #removed else \"\"\n",
    "print(len(updated_containers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1483e6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_content=pd.DataFrame([updated_containers]).T\n",
    "dataframe_content.columns=[\"html_content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa13c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5136b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_cleaner(inputs):\n",
    "    \"\"\"\n",
    "    This function takes in the content with HTML and removes tags\n",
    "    and emojis\n",
    "    \"\"\"\n",
    "    cleaned_content=[]\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U00010000-\\U0010ffff\"\n",
    "        u\"\\u2640-\\u2642\" \n",
    "        u\"\\u2600-\\u2B55\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u23cf\"\n",
    "        u\"\\u23e9\"\n",
    "        u\"\\u231a\"\n",
    "        u\"\\ufe0f\"  # dingbats\n",
    "        u\"\\u3030\"\n",
    "                          \"]+\", flags = re.UNICODE)\n",
    "    cleanr = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    for items in inputs:\n",
    "        cleantext = re.sub(cleanr, \"\", str(items))\n",
    "        cleantext=cleantext.replace('\\n',\"\")\n",
    "        cleantext=re.sub(regrex_pattern,\"\" ,cleantext)\n",
    "        cleaned_content.append(str(cleantext).lower())  \n",
    "    return cleaned_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfc6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get span for each row\n",
    "text_content=[]\n",
    "for n in range(len(dataframe_content)):\n",
    "    each_row_soup=bs(str(dataframe_content[\"html_content\"][n]))\n",
    "    containers_span_each_row = each_row_soup.findAll(\"span\",{\"class\":\"break-words\"})\n",
    "    containers_span_each_row=complete_cleaner(containers_span_each_row)\n",
    "    text_content.append(containers_span_each_row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eadc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add the cleaned content to the dataframe\n",
    "dataframe_content[\"Text\"]=text_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09a1976",
   "metadata": {},
   "source": [
    "## 2.Classify content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a89991",
   "metadata": {},
   "source": [
    "# Load the model to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb21416",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the classifier pipeline\n",
    "with open('pipeline_classifier.pickle', 'rb') as file:\n",
    "    pipeline_classifier = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810d07a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the predicted tags to Label column in the dataframe\n",
    "dataframe_content[\"Label\"]=pipeline_classifier.predict(dataframe_content[\"Text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb86c2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check to ensure Labels are available\n",
    "dataframe_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbff9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Check the distribution of content in your feed\n",
    "sns.set()\n",
    "sns.countplot(x='Label', data=dataframe_content)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e888d11a",
   "metadata": {},
   "source": [
    "## 3.Access the required category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714e4663",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to return the html\n",
    "def select_label(label):\n",
    "    if label==1:\n",
    "        container_x=\" \".join(str(x) for x in dataframe_content.loc[dataframe_content[\"Label\"]==\"Job lead\"][\"html_content\"])\n",
    "        display(HTML(container_x))\n",
    "    elif label==2:\n",
    "        container_x=\" \".join(str(x) for x in dataframe_content.loc[dataframe_content[\"Label\"]==\"Event\"][\"html_content\"])\n",
    "        display(HTML(str(container_x)))\n",
    "    elif label==3:\n",
    "        container_x=\" \".join(str(x) for x in dataframe_content.loc[dataframe_content[\"Label\"]==\"Others\"][\"html_content\"])\n",
    "        display(HTML(str(container_x)))\n",
    "    elif label==4:\n",
    "        container_x=\" \".join(str(x) for x in dataframe_content.loc[dataframe_content[\"Label\"]==\"Advertisement\"][\"html_content\"])\n",
    "        display(HTML(str(container_x)))\n",
    "    else:\n",
    "        return print(\"Select label 1,2,3 or 4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da1bcb1",
   "metadata": {},
   "source": [
    "## Use the select label function to pick the desired content. \n",
    "    For Job leads, use select_label(1)\n",
    "    For Events, use select_label(2)\n",
    "    For Others, use select_label(3)\n",
    "    For Advertisements, use select_label(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d599e2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#For example, to browse job leads use\n",
    "select_label(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd05de7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
